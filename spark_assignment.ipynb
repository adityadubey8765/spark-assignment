{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e53ae6-a267-4e52-9a75-8d7f1a61c405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting sqlalchemy\n  Downloading sqlalchemy-2.0.41-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\nCollecting typing-extensions>=4.6.0\n  Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\nCollecting greenlet>=1\n  Downloading greenlet-3.2.2-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (579 kB)\nInstalling collected packages: typing-extensions, greenlet, sqlalchemy\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-afe2b8f3-2b1c-4d20-88bb-00701520766e\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed greenlet-3.2.2 sqlalchemy-2.0.41 typing-extensions-4.14.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76e271a-10d3-4d1c-be14-8096715219d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transaction_pattern_detection.py\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import psycopg2\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7488930-14a8-4c36-b110-37ad8914b9a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, percent_rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3bdd000-c6b4-43bc-98ca-7d1aaf27df8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Below file contains the security keys and other secret information needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e66633-f88f-4660-ad73-83e99891e6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./aws_postgres_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d0cdd2-57fa-40f8-abb3-d0034069a09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "S3_BUCKET = 'my-databricks-bucket45'\n",
    "S3_INPUT_PREFIX = 'input_chunks/'\n",
    "S3_DETECTION_PREFIX = 'detections/'\n",
    "TRANSACTIONS_S3_PATH = f's3a://{S3_BUCKET}/data/transactions.csv'\n",
    "IMPORTANCE_S3_PATH = f's3a://{S3_BUCKET}/data/CustomerImportance.csv'\n",
    "CHUNK_SIZE = 10000\n",
    "CHUNK_INTERVAL = 1  # seconds\n",
    "\n",
    "# AWS RDS PostgreSQL credentials\n",
    "RDS_PORT = '5432'\n",
    "RDS_DBNAME = 'spark'\n",
    "POSTGRES_URI = f'postgresql://{RDS_USER}:{RDS_PASSWORD}@{RDS_HOST}:{RDS_PORT}/{RDS_DBNAME}'\n",
    "\n",
    "# INIT\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransactionPatternDetection\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.session.token\", AWS_SESSION_TOKEN) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=AWS_SESSION_TOKEN if AWS_SESSION_TOKEN else None\n",
    ")\n",
    "pg_engine = create_engine(POSTGRES_URI)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f5ce28-efbf-455c-bd08-e22432390ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk 0 to S3: input_chunks/transactions_chunk_20250605165232.csv\nUploaded chunk 10000 to S3: input_chunks/transactions_chunk_20250605165234.csv\nUploaded chunk 20000 to S3: input_chunks/transactions_chunk_20250605165235.csv\nUploaded chunk 30000 to S3: input_chunks/transactions_chunk_20250605165237.csv\nUploaded chunk 40000 to S3: input_chunks/transactions_chunk_20250605165238.csv\nUploaded chunk 50000 to S3: input_chunks/transactions_chunk_20250605165240.csv\nUploaded chunk 60000 to S3: input_chunks/transactions_chunk_20250605165241.csv\nUploaded chunk 70000 to S3: input_chunks/transactions_chunk_20250605165243.csv\nUploaded chunk 80000 to S3: input_chunks/transactions_chunk_20250605165244.csv\nUploaded chunk 90000 to S3: input_chunks/transactions_chunk_20250605165246.csv\nUploaded chunk 100000 to S3: input_chunks/transactions_chunk_20250605165247.csv\nUploaded chunk 110000 to S3: input_chunks/transactions_chunk_20250605165249.csv\nUploaded chunk 120000 to S3: input_chunks/transactions_chunk_20250605165250.csv\nUploaded chunk 130000 to S3: input_chunks/transactions_chunk_20250605165252.csv\nUploaded chunk 140000 to S3: input_chunks/transactions_chunk_20250605165253.csv\nProcessed 4 files together.\nUploaded chunk 150000 to S3: input_chunks/transactions_chunk_20250605165255.csv\nUploaded chunk 160000 to S3: input_chunks/transactions_chunk_20250605165256.csv\nUploaded chunk 170000 to S3: input_chunks/transactions_chunk_20250605165258.csv\nUploaded chunk 180000 to S3: input_chunks/transactions_chunk_20250605165300.csv\nUploaded chunk 190000 to S3: input_chunks/transactions_chunk_20250605165301.csv\nUploaded chunk 200000 to S3: input_chunks/transactions_chunk_20250605165303.csv\nUploaded chunk 210000 to S3: input_chunks/transactions_chunk_20250605165304.csv\nUploaded chunk 220000 to S3: input_chunks/transactions_chunk_20250605165306.csv\nUploaded chunk 230000 to S3: input_chunks/transactions_chunk_20250605165307.csv\nUploaded chunk 240000 to S3: input_chunks/transactions_chunk_20250605165309.csv\nUploaded chunk 250000 to S3: input_chunks/transactions_chunk_20250605165310.csv\nUploaded chunk 260000 to S3: input_chunks/transactions_chunk_20250605165312.csv\nUploaded chunk 270000 to S3: input_chunks/transactions_chunk_20250605165313.csv\nUploaded chunk 280000 to S3: input_chunks/transactions_chunk_20250605165315.csv\nUploaded chunk 290000 to S3: input_chunks/transactions_chunk_20250605165316.csv\nUploaded chunk 300000 to S3: input_chunks/transactions_chunk_20250605165318.csv\nUploaded chunk 310000 to S3: input_chunks/transactions_chunk_20250605165319.csv\nUploaded chunk 320000 to S3: input_chunks/transactions_chunk_20250605165321.csv\nUploaded chunk 330000 to S3: input_chunks/transactions_chunk_20250605165323.csv\nUploaded chunk 340000 to S3: input_chunks/transactions_chunk_20250605165324.csv\nUploaded chunk 350000 to S3: input_chunks/transactions_chunk_20250605165326.csv\nUploaded chunk 360000 to S3: input_chunks/transactions_chunk_20250605165327.csv\nUploaded detections_20250605165328002376.csv\nProcessed 11 files together.\nUploaded chunk 370000 to S3: input_chunks/transactions_chunk_20250605165329.csv\nUploaded chunk 380000 to S3: input_chunks/transactions_chunk_20250605165330.csv\nUploaded chunk 390000 to S3: input_chunks/transactions_chunk_20250605165332.csv\nUploaded chunk 400000 to S3: input_chunks/transactions_chunk_20250605165333.csv\nUploaded chunk 410000 to S3: input_chunks/transactions_chunk_20250605165334.csv\nUploaded chunk 420000 to S3: input_chunks/transactions_chunk_20250605165336.csv\nUploaded chunk 430000 to S3: input_chunks/transactions_chunk_20250605165337.csv\nUploaded chunk 440000 to S3: input_chunks/transactions_chunk_20250605165339.csv\nUploaded chunk 450000 to S3: input_chunks/transactions_chunk_20250605165340.csv\nUploaded chunk 460000 to S3: input_chunks/transactions_chunk_20250605165342.csv\nUploaded chunk 470000 to S3: input_chunks/transactions_chunk_20250605165343.csv\nUploaded chunk 480000 to S3: input_chunks/transactions_chunk_20250605165345.csv\nUploaded chunk 490000 to S3: input_chunks/transactions_chunk_20250605165346.csv\nUploaded chunk 500000 to S3: input_chunks/transactions_chunk_20250605165348.csv\nUploaded chunk 510000 to S3: input_chunks/transactions_chunk_20250605165349.csv\nUploaded chunk 520000 to S3: input_chunks/transactions_chunk_20250605165351.csv\nUploaded chunk 530000 to S3: input_chunks/transactions_chunk_20250605165352.csv\nUploaded chunk 540000 to S3: input_chunks/transactions_chunk_20250605165354.csv\nUploaded chunk 550000 to S3: input_chunks/transactions_chunk_20250605165355.csv\nUploaded chunk 560000 to S3: input_chunks/transactions_chunk_20250605165357.csv\nUploaded chunk 570000 to S3: input_chunks/transactions_chunk_20250605165359.csv\nUploaded chunk 580000 to S3: input_chunks/transactions_chunk_20250605165400.csv\nUploaded chunk 590000 to S3: input_chunks/transactions_chunk_20250605165402.csv\nUploaded detections_20250605165420026792.csv\nProcessed 22 files together.\nUploaded detections_20250605165511673147.csv\nProcessed 23 files together.\nAll files processed. Exiting.\n"
     ]
    }
   ],
   "source": [
    "class ChunkUploader:\n",
    "    def __init__(self):\n",
    "        self.offset = 0\n",
    "        response = s3.get_object(Bucket=S3_BUCKET, Key=\"transactions.csv\")\n",
    "        self.df = pd.read_csv(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "    def upload_chunk(self):\n",
    "        with threading.Lock():\n",
    "            if self.offset >= len(self.df):\n",
    "                return\n",
    "\n",
    "            chunk = self.df.iloc[self.offset:self.offset + CHUNK_SIZE]\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            local_path = f\"/tmp/transactions_chunk_{timestamp}.csv\"\n",
    "            chunk.to_csv(local_path, index=False)\n",
    "\n",
    "            s3_key = f\"{S3_INPUT_PREFIX}transactions_chunk_{timestamp}.csv\"\n",
    "            s3.upload_file(local_path, S3_BUCKET, s3_key)\n",
    "            print(f\"Uploaded chunk {self.offset} to S3: {s3_key}\")\n",
    "\n",
    "            self.offset += CHUNK_SIZE\n",
    "        threading.Timer(CHUNK_INTERVAL, self.upload_chunk).start()\n",
    "\n",
    "\n",
    "# MECHANISM Y\n",
    "\n",
    "def detect_patterns(transactions_df):\n",
    "    detections = []\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    df = transactions_df.withColumn(\"Amount\", col(\"amount\").cast(\"float\"))\n",
    "\n",
    "    # Store merchant tx count to PostgreSQL\n",
    "    merchant_counts = df.groupBy(\"merchant\").count()\n",
    "    merchant_counts_df = merchant_counts.toPandas()\n",
    "    merchant_counts_df.to_sql(\"merchant_txn_counts\", pg_engine, if_exists='replace', index=False)\n",
    "\n",
    "    # PatId1\n",
    "    merchant_txn_counts = transactions_df.groupBy(\"merchant\").count().withColumnRenamed(\"count\", \"merchant_txn_count\")\n",
    "    active_merchants = merchant_txn_counts.filter(col(\"merchant_txn_count\") > 50000)\n",
    "    filtered_txns = transactions_df.join(active_merchants, on=\"merchant\", how=\"inner\")\n",
    "    customer_txn_counts = filtered_txns.groupBy(\"merchant\", \"customer\") \\\n",
    "                                       .agg(count(\"*\").alias(\"txn_count\"), sum(\"amount\").alias(\"total_weight\"))\n",
    "    txn_rank_window = Window.partitionBy(\"merchant\").orderBy(col(\"txn_count\").desc())\n",
    "    ranked_customers = customer_txn_counts \\\n",
    "        .withColumn(\"txn_rank\", percent_rank().over(txn_rank_window))\n",
    "    pattern_df = ranked_customers \\\n",
    "        .filter((col(\"txn_rank\") <= 0.01))\n",
    "    weight_rank_window = Window.partitionBy(\"merchant\").orderBy(col(\"total_weight\").asc())\n",
    "    ranked_customers_1 = pattern_df \\\n",
    "        .withColumn(\"weight_rank\", percent_rank().over(weight_rank_window))\n",
    "    pattern_df_1 = ranked_customers_1 \\\n",
    "        .filter((col(\"weight_rank\") <= 0.01)) \\\n",
    "        .withColumn(\"actionType\", lit(\"UPGRADE\"))\n",
    "\n",
    "    for row in pattern_df_1.select(col(\"customer\"), col(\"merchant\")).distinct().collect():\n",
    "            detections.append((current_time, current_time, \"PatId1\", \"UPGRADE\", row['customer'], row['merchant']))\n",
    "\n",
    "    # PatId2\n",
    "    avg_txn = transactions_df.groupBy(\"customer\", \"merchant\") \\\n",
    "        .agg(avg(\"Amount\").alias(\"avg_amt\"), count(\"*\").alias(\"txn_count\")) \\\n",
    "        .filter((col(\"avg_amt\") < 23) & (col(\"txn_count\") >= 80))\n",
    "    for row in avg_txn.collect():\n",
    "        detections.append((current_time, current_time, \"PatId2\", \"CHILD\", row['customer'], row['merchant']))\n",
    "\n",
    "    # PatId3\n",
    "    gender_df = transactions_df.groupBy(\"merchant\", \"gender\").count().groupBy(\"merchant\").pivot(\"gender\").sum(\"count\").na.fill(0)\n",
    "    gender_filtered = gender_df.filter((col(\"F\") < col(\"M\")) & (col(\"F\") > 100))\n",
    "    for row in gender_filtered.select(\"merchant\").collect():\n",
    "        detections.append((current_time, current_time, \"PatId3\", \"DEI-NEEDED\", \"\", row['merchant']))\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "def process_stream():\n",
    "    obj = s3.get_object(Bucket=S3_BUCKET, Key='CustomerImportance.csv')\n",
    "    importance_pd_df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    importance_df = spark.createDataFrame(importance_pd_df)\n",
    "\n",
    "    # Clean quotes and rename columns\n",
    "    for col_name in ['Source', 'Target', 'typeTrans']:\n",
    "        importance_df = importance_df.withColumn(col_name, regexp_replace(col_name, \"'\", \"\"))\n",
    "    importance_df = importance_df.withColumnRenamed(\"Source\", \"customer\") \\\n",
    "                                 .withColumnRenamed(\"Target\", \"merchant\") \\\n",
    "                                 .withColumnRenamed(\"typeTrans\", \"category\")\n",
    "\n",
    "    processed_files = set()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            objects = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=S3_INPUT_PREFIX).get('Contents', [])\n",
    "            new_files = [o['Key'] for o in objects if not o['Key'].endswith('/')]\n",
    "\n",
    "            \n",
    "            unprocessed_files = [key for key in new_files if key not in processed_files]\n",
    "\n",
    "            if not unprocessed_files:\n",
    "                print(\"All files processed. Exiting.\")\n",
    "                break\n",
    "\n",
    "            # Combine all unprocessed files into one DataFrame\n",
    "            dfs = []\n",
    "            for key in unprocessed_files:\n",
    "                obj = s3.get_object(Bucket=S3_BUCKET, Key=key)\n",
    "                df_pd = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "                dfs.append(spark.createDataFrame(df_pd))\n",
    "\n",
    "            # Combine all Spark DataFrames into one\n",
    "            combined_df = dfs[0]\n",
    "            for df_part in dfs[1:]:\n",
    "                combined_df = combined_df.unionByName(df_part)\n",
    "\n",
    "            # Clean customer, merchant, category\n",
    "            for col_name in ['customer', 'age', 'gender','zipcodeOri','merchant','zipMerchant','category']:\n",
    "                combined_df = combined_df.withColumn(col_name, regexp_replace(col_name, \"'\", \"\"))\n",
    "\n",
    "            detections = detect_patterns(combined_df)\n",
    "\n",
    "            for i in range(0, len(detections), 50):\n",
    "                batch = detections[i:i + 50]\n",
    "                out_df = pd.DataFrame(batch, columns=[\"YStartTime\", \"detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"MerchantId\"])\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "                local_file = f\"/tmp/detections_{timestamp}.csv\"\n",
    "                out_df.to_csv(local_file, index=False)\n",
    "                s3.upload_file(local_file, S3_BUCKET, f\"{S3_DETECTION_PREFIX}detections_{timestamp}.csv\")\n",
    "                print(f\"Uploaded detections_{timestamp}.csv\")\n",
    "\n",
    "            # Mark all files as processed\n",
    "            processed_files.update(unprocessed_files)\n",
    "            print(f\"Processed {len(unprocessed_files)} files together.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Error in stream processing\")\n",
    "            time.sleep(10)\n",
    "\n",
    "# MAIN\n",
    "if __name__ == '__main__':\n",
    "    uploader = ChunkUploader()\n",
    "    uploader.upload_chunk()\n",
    "    process_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c438c7d-3664-4bd7-8e94-967b3a3d1e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded detections_20250605170507240693.csv\nProcessed 60 files together.\nAll files processed. Exiting.\n"
     ]
    }
   ],
   "source": [
    "class ChunkUploader:\n",
    "    def __init__(self):\n",
    "        self.offset = 0\n",
    "        response = s3.get_object(Bucket=S3_BUCKET, Key=\"transactions.csv\")\n",
    "        self.df = pd.read_csv(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "    def upload_chunk(self):\n",
    "        with threading.Lock():\n",
    "            if self.offset >= len(self.df):\n",
    "                return\n",
    "\n",
    "            chunk = self.df.iloc[self.offset:self.offset + CHUNK_SIZE]\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            local_path = f\"/tmp/transactions_chunk_{timestamp}.csv\"\n",
    "            chunk.to_csv(local_path, index=False)\n",
    "\n",
    "            s3_key = f\"{S3_INPUT_PREFIX}transactions_chunk_{timestamp}.csv\"\n",
    "            s3.upload_file(local_path, S3_BUCKET, s3_key)\n",
    "            print(f\"Uploaded chunk {self.offset} to S3: {s3_key}\")\n",
    "\n",
    "            self.offset += CHUNK_SIZE\n",
    "        threading.Timer(CHUNK_INTERVAL, self.upload_chunk).start()\n",
    "\n",
    "\n",
    "# MECHANISM Y\n",
    "\n",
    "def detect_patterns(transactions_df):\n",
    "    detections = []\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    df = transactions_df.withColumn(\"Amount\", col(\"amount\").cast(\"float\"))\n",
    "\n",
    "    # Store merchant tx count to PostgreSQL\n",
    "    merchant_counts = df.groupBy(\"merchant\").count()\n",
    "    merchant_counts_df = merchant_counts.toPandas()\n",
    "    merchant_counts_df.to_sql(\"merchant_txn_counts\", pg_engine, if_exists='replace', index=False)\n",
    "\n",
    "    # PatId1\n",
    "    merchant_txn_counts = transactions_df.groupBy(\"merchant\").count().withColumnRenamed(\"count\", \"merchant_txn_count\")\n",
    "    active_merchants = merchant_txn_counts.filter(col(\"merchant_txn_count\") > 50000)\n",
    "    filtered_txns = transactions_df.join(active_merchants, on=\"merchant\", how=\"inner\")\n",
    "    customer_txn_counts = filtered_txns.groupBy(\"merchant\", \"customer\") \\\n",
    "                                       .agg(count(\"*\").alias(\"txn_count\"), sum(\"amount\").alias(\"total_weight\"))\n",
    "    txn_rank_window = Window.partitionBy(\"merchant\").orderBy(col(\"txn_count\").desc())\n",
    "    ranked_customers = customer_txn_counts \\\n",
    "        .withColumn(\"txn_rank\", percent_rank().over(txn_rank_window))\n",
    "    pattern_df = ranked_customers \\\n",
    "        .filter((col(\"txn_rank\") <= 0.01))\n",
    "    weight_rank_window = Window.partitionBy(\"merchant\").orderBy(col(\"total_weight\").asc())\n",
    "    ranked_customers_1 = pattern_df \\\n",
    "        .withColumn(\"weight_rank\", percent_rank().over(weight_rank_window))\n",
    "    pattern_df_1 = ranked_customers_1 \\\n",
    "        .filter((col(\"weight_rank\") <= 0.01)) \\\n",
    "        .withColumn(\"actionType\", lit(\"UPGRADE\"))\n",
    "\n",
    "    for row in pattern_df_1.select(col(\"customer\"), col(\"merchant\")).distinct().collect():\n",
    "            detections.append((current_time, current_time, \"PatId1\", \"UPGRADE\", row['customer'], row['merchant']))\n",
    "\n",
    "    # PatId2\n",
    "    avg_txn = transactions_df.groupBy(\"customer\", \"merchant\") \\\n",
    "        .agg(avg(\"Amount\").alias(\"avg_amt\"), count(\"*\").alias(\"txn_count\")) \\\n",
    "        .filter((col(\"avg_amt\") < 23) & (col(\"txn_count\") >= 80))\n",
    "    for row in avg_txn.collect():\n",
    "        detections.append((current_time, current_time, \"PatId2\", \"CHILD\", row['customer'], row['merchant']))\n",
    "\n",
    "    # PatId3\n",
    "    gender_df = transactions_df.groupBy(\"merchant\", \"gender\").count().groupBy(\"merchant\").pivot(\"gender\").sum(\"count\").na.fill(0)\n",
    "    gender_filtered = gender_df.filter((col(\"F\") < col(\"M\")) & (col(\"F\") > 100))\n",
    "    for row in gender_filtered.select(\"merchant\").collect():\n",
    "        detections.append((current_time, current_time, \"PatId3\", \"DEI-NEEDED\", \"\", row['merchant']))\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "def process_stream():\n",
    "    obj = s3.get_object(Bucket=S3_BUCKET, Key='CustomerImportance.csv')\n",
    "    importance_pd_df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    importance_df = spark.createDataFrame(importance_pd_df)\n",
    "\n",
    "    # Clean quotes and rename columns\n",
    "    for col_name in ['Source', 'Target', 'typeTrans']:\n",
    "        importance_df = importance_df.withColumn(col_name, regexp_replace(col_name, \"'\", \"\"))\n",
    "    importance_df = importance_df.withColumnRenamed(\"Source\", \"customer\") \\\n",
    "                                 .withColumnRenamed(\"Target\", \"merchant\") \\\n",
    "                                 .withColumnRenamed(\"typeTrans\", \"category\")\n",
    "\n",
    "    processed_files = set()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            objects = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=S3_INPUT_PREFIX).get('Contents', [])\n",
    "            new_files = [o['Key'] for o in objects if not o['Key'].endswith('/')]\n",
    "\n",
    "\n",
    "            if len(new_files)==60:\n",
    "\n",
    "            \n",
    "                unprocessed_files = [key for key in new_files if key not in processed_files]\n",
    "\n",
    "                if not unprocessed_files:\n",
    "                    print(\"All files processed. Exiting.\")\n",
    "                    break\n",
    "\n",
    "                # Combine all unprocessed files into one DataFrame\n",
    "                dfs = []\n",
    "                for key in unprocessed_files:\n",
    "                    obj = s3.get_object(Bucket=S3_BUCKET, Key=key)\n",
    "                    df_pd = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "                    dfs.append(spark.createDataFrame(df_pd))\n",
    "\n",
    "                # Combine all Spark DataFrames into one\n",
    "                combined_df = dfs[0]\n",
    "                for df_part in dfs[1:]:\n",
    "                    combined_df = combined_df.unionByName(df_part)\n",
    "\n",
    "                # Clean customer, merchant, category\n",
    "                for col_name in ['customer', 'age', 'gender','zipcodeOri','merchant','zipMerchant','category']:\n",
    "                    combined_df = combined_df.withColumn(col_name, regexp_replace(col_name, \"'\", \"\"))\n",
    "\n",
    "                detections = detect_patterns(combined_df)\n",
    "\n",
    "                for i in range(0, len(detections), 50):\n",
    "                    batch = detections[i:i + 50]\n",
    "                    out_df = pd.DataFrame(batch, columns=[\"YStartTime\", \"detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"MerchantId\"])\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "                    local_file = f\"/tmp/detections_{timestamp}.csv\"\n",
    "                    out_df.to_csv(local_file, index=False)\n",
    "                    s3.upload_file(local_file, S3_BUCKET, f\"{S3_DETECTION_PREFIX}detections_{timestamp}.csv\")\n",
    "                    print(f\"Uploaded detections_{timestamp}.csv\")\n",
    "\n",
    "                # Mark all files as processed\n",
    "                processed_files.update(unprocessed_files)\n",
    "                print(f\"Processed {len(unprocessed_files)} files together.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Error in stream processing\")\n",
    "            time.sleep(10)\n",
    "\n",
    "# MAIN\n",
    "if __name__ == '__main__':\n",
    "    # uploader = ChunkUploader()\n",
    "    # uploader.upload_chunk()\n",
    "    process_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20c410d-d650-4efa-806a-b14387c52766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ZIP: /tmp/s3_zip_temp/output.zip\nUploaded ZIP to s3://my-databricks-bucket45/zipped_output/output.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "folder1 = 'input_chunks/'\n",
    "folder2 = 'detections/'\n",
    "output_file = 'output.zip'\n",
    "OUTPUT_S3_KEY = f'zipped_output/{output_file}'\n",
    "\n",
    "# Use /tmp for temporary storage\n",
    "LOCAL_TMP_DIR = '/tmp/s3_zip_temp'\n",
    "ZIP_PATH = f'{LOCAL_TMP_DIR}/{output_file}'\n",
    "\n",
    "# Create temp folder\n",
    "os.makedirs(LOCAL_TMP_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def download_folder(prefix):\n",
    "    response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)\n",
    "    if 'Contents' not in response:\n",
    "        print(f\"No objects found in {prefix}\")\n",
    "        return []\n",
    "\n",
    "    downloaded_files = []\n",
    "\n",
    "    for obj in response['Contents']:\n",
    "        key = obj['Key']\n",
    "        if key.endswith('/'):  # Skip folder \"keys\"\n",
    "            continue\n",
    "\n",
    "        file_name = key.split('/')[-1]\n",
    "        local_file_path = os.path.join(LOCAL_TMP_DIR, file_name)\n",
    "\n",
    "        s3.download_file(S3_BUCKET, key, local_file_path)\n",
    "        downloaded_files.append(local_file_path)\n",
    "\n",
    "    return downloaded_files\n",
    "\n",
    "# Download both folders\n",
    "files1 = download_folder(folder1)\n",
    "files2 = download_folder(folder2)\n",
    "\n",
    "# Combine and zip\n",
    "with zipfile.ZipFile(ZIP_PATH, 'w') as zipf:\n",
    "    for file_path in files1 + files2:\n",
    "        arcname = os.path.basename(file_path)  # Store just filename\n",
    "        zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"Created ZIP: {ZIP_PATH}\")\n",
    "\n",
    "# Upload zip back to S3\n",
    "s3.upload_file(ZIP_PATH, S3_BUCKET, OUTPUT_S3_KEY)\n",
    "print(f\"Uploaded ZIP to s3://{S3_BUCKET}/{OUTPUT_S3_KEY}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}